TECHNOVA AI SYSTEMS - INTERNAL SLACK CHANNEL
Channel: #ml-engineering-private
Date Range: July 15-22, 2024
Export Date: September 10, 2024
Classification: Internal Communications - Confidential

================================================================================
CHANNEL INFO
================================================================================
Channel: #ml-engineering-private
Purpose: ML engineering team technical discussions
Members: 12 (ML Engineers, Data Scientists, AI Research Team)
Privacy: Private channel - invite only

EXPORT NOTE: This export was requested by Legal Department as part of internal
investigation and potential litigation preparation. Messages may be subject to
attorney-client privilege and/or attorney work product doctrine.

================================================================================
JULY 15, 2024
================================================================================

[10:23 AM] sofia.rodriguez
Hey team, quick question about our training data sources for the performance
prediction model. I'm looking at the data provenance docs and I'm seeing we have
historical performance data from 85 client orgs going back to 2015. Do we have
any documentation about whether those orgs had bias issues during those periods?

[10:31 AM] james.kim
Good question. I don't think we systematically checked for that. We did basic
data quality checks (completeness, consistency) but not bias audits of the source
orgs.

[10:33 AM] james.kim
Why do you ask?

[10:35 AM] sofia.rodriguez
I've been thinking about Article 10 of the AI Act - it requires training data to
be examined for "possible biases". If our training data includes performance
ratings from periods when orgs had gender pay gaps or promotion disparities, we
might be training our model to perpetuate those biases.

[10:37 AM] maria.santos
Oh wow, that's a really good point. I hadn't thought about that.

[10:38 AM] maria.santos
We definitely have some data from before a lot of orgs implemented their DEI
programs. Like, I know one of our big banking clients had a discrimination lawsuit
in 2017. If we're using their pre-2017 performance data...

[10:41 AM] sofia.rodriguez
Exactly. And the model doesn't know which ratings were fair vs biased. It just
learns the patterns. So if women got lower ratings historically (even unfairly),
the model learns to predict lower ratings for women.

[10:43 AM] alex.chen
This is the whole "garbage in, garbage out" problem. But how do we even fix this?
We can't go back in time and re-do historical performance reviews.

[10:45 AM] sofia.rodriguez
A few options:
1. Exclude data from periods with known bias issues
2. Use debiasing techniques (reweighting, relabeling)
3. Add fairness constraints in model training
4. Some combination of above

But first we need to actually analyze whether we have this problem or if I'm just
being paranoid.

[10:48 AM] james.kim
Not paranoid. This is exactly the kind of thing regulators will look at under AI Act.

[10:49 AM] james.kim
We should probably do this analysis ASAP. Want to set up a meeting with Michael
[Dr. Zhang]?

[10:51 AM] sofia.rodriguez
Yeah, I'll ping him. Can you pull together the training data metadata - which
orgs, which time periods, what's the demographic breakdown?

[10:52 AM] james.kim
On it. Will have something by end of week.

================================================================================
JULY 17, 2024
================================================================================

[2:14 PM] alex.chen
So I was looking at our resume scoring model's training data and noticed something
weird. We have 312,000 resume-job pairs with recruiter relevance scores, right?

[2:16 PM] alex.chen
I ran some quick stats on the resumes. Here's the gender breakdown (inferred from
names):
- Male names: 68%
- Female names: 29%
- Ambiguous: 3%

For senior technical roles specifically:
- Male names: 81%
- Female names: 17%
- Ambiguous: 2%

Is this representative of actual applicant pools, or is this showing historical
gender imbalance in tech?

[2:22 PM] maria.santos
Probably both? Tech has historically been male-dominated, so actual applicant
pools skewed male. But 81% for senior roles seems extreme even for tech.

[2:24 PM] sofia.rodriguez
This is related to what I was talking about on Monday. If our training data
reflects historical bias, our model might learn to prefer male candidates even
when qualifications are equal.

[2:25 PM] sofia.rodriguez
Did you look at the relevance scores? Are male resumes scored higher by recruiters
on average?

[2:28 PM] alex.chen
Let me check...

[2:35 PM] alex.chen
Okay this is interesting. For identical job descriptions:
- Male resumes: average recruiter score 72.3
- Female resumes: average recruiter score 69.8

Difference is statistically significant (p < 0.001)

But this could be because male resumes on average have more experience? Let me
control for years of experience...

[2:44 PM] alex.chen
Okay, controlling for years of experience, the gap shrinks but doesn't disappear:
- Male resumes: 71.8
- Female resumes: 70.2

Still significant at p < 0.05

So our training labels (recruiter scores) appear to have some gender bias baked in.

[2:47 PM] sofia.rodriguez
ðŸ˜¬

[2:48 PM] maria.santos
This is not good. If we train our model on biased human labels, the model learns
to be biased.

[2:50 PM] james.kim
Devil's advocate: Maybe the male resumes really are slightly better on average
due to more experience in field? The tech pipeline has been male-dominated for
decades.

[2:52 PM] sofia.rodriguez
That's possible, but it's also possible the recruiters were biased - consciously
or unconsciously. Research shows identical resumes with male vs female names get
different ratings.

[2:53 PM] sofia.rodriguez
Either way, we have a problem. If we're replicating historical bias, we're
violating AI Act Article 10. If we're actually making fair predictions but they
happen to disadvantage women due to historical factors, we have a fairness and
optics problem.

[2:55 PM] alex.chen
So what do we do?

[2:57 PM] sofia.rodriguez
I think we need to:
1. Run proper bias testing on our trained models (not just the training data)
2. Loop in Elena [Dr. KovÃ¡cs] from AI Ethics
3. Probably loop in Legal too
4. Figure out mitigation strategy

[2:59 PM] james.kim
I'll bring this up in our 1:1 with Michael tomorrow. This seems like a
leadership-level decision, not just an engineering problem.

[3:01 PM] maria.santos
Agreed. We need top-down support for this. It might require redoing a lot of work.

[3:02 PM] sofia.rodriguez
Better to find it now than have regulators or clients find it later.

[3:03 PM] alex.chen
True. Okay, I'll document my analysis and share with Michael and Elena.

================================================================================
JULY 19, 2024
================================================================================

[11:47 AM] james.kim
Update from my meeting with Michael: he wants us to do comprehensive bias testing
across all our models. He's setting up a task force.

[11:49 AM] sofia.rodriguez
Good. What's the scope?

[11:52 AM] james.kim
All production models:
- Resume scoring
- Performance prediction
- Turnover risk
- Skills extraction
- Compensation recommendations

Test for bias on:
- Gender (inferred from names)
- Age (from date of birth where available)
- Ethnicity/national origin (inferred from names, with caveats)

[11:54 AM] maria.santos
That's a lot of work. Timeline?

[11:56 AM] james.kim
Michael wants preliminary results in 2 weeks. Comprehensive report in 4 weeks.

[11:57 AM] alex.chen
Who's leading this?

[11:58 AM] james.kim
Sofia, if you're willing. Michael thinks you have the right combination of
technical depth and ethics awareness.

[12:02 PM] sofia.rodriguez
I'm willing but I'm nervous. If we find serious bias (which I suspect we will),
this could blow up into a major issue.

[12:04 PM] james.kim
Michael is aware. He said "better to know and fix than be surprised by external
audit or regulator."

[12:06 PM] sofia.rodriguez
Okay. I'm in. Can I get Alex and Maria supporting on this?

[12:07 PM] alex.chen
Yep, I'm in.

[12:08 PM] maria.santos
Same. This is important.

[12:10 PM] sofia.rodriguez
Alright team. Let's find out how biased our models actually are. I'll create a
project plan and share this afternoon.

[12:11 PM] sofia.rodriguez
And let's be really careful about documenting this. Everything we find might end
up in regulatory filings or litigation discovery. Write like lawyers might read it.

[12:13 PM] james.kim
Good point. Maybe we should have our analysis reviewed by Legal before we
circulate widely?

[12:15 PM] sofia.rodriguez
Yeah. I'll check with Michael on the protocol.

================================================================================
JULY 22, 2024
================================================================================

[4:33 PM] sofia.rodriguez
Quick update for the team: I've started the bias testing. Early results are...
concerning.

[4:35 PM] alex.chen
How concerning?

[4:37 PM] sofia.rodriguez
I tested the resume scoring model with synthetic resumes (identical qualifications,
only name changed). Getting 7+ point score differential between male and female
names.

[4:38 PM] maria.santos
ðŸ˜¬ðŸ˜¬ðŸ˜¬

[4:39 PM] alex.chen
That's really bad. That's the kind of thing that makes headlines: "AI Discriminates
Against Women"

[4:41 PM] sofia.rodriguez
Yeah. And it's not just gender. I'm seeing ethnicity effects too. Names that
sound African, Middle Eastern, or Eastern European score lower than Western
European names with identical qualifications.

[4:43 PM] james.kim
Have you told Michael yet?

[4:45 PM] sofia.rodriguez
I'm documenting everything first. I want to have rigorous methodology and clear
results before I escalate. Should have preliminary report done by end of week.

[4:46 PM] sofia.rodriguez
But I wanted to give you all a heads up because this is going to become a big deal.

[4:48 PM] maria.santos
What do you think happens when you report this?

[4:51 PM] sofia.rodriguez
Best case: We implement fixes, deploy debiased models, add fairness constraints,
improve our processes. Maybe some cost and delay but we get better.

[4:52 PM] sofia.rodriguez
Worst case: This becomes a legal/regulatory issue. Maybe we have to notify clients,
maybe we face regulatory scrutiny, maybe affected people file complaints.

[4:54 PM] alex.chen
Do we have to report ourselves to regulators if we find bias?

[4:56 PM] sofia.rodriguez
I don't know. That's a legal question. But I think even if we're not legally
required to, we probably should notify clients at minimum since they're using our
system to make employment decisions.

[4:58 PM] james.kim
This is above our pay grade. We do the technical analysis, document rigorously,
report to leadership, and let them make the strategic calls.

[5:00 PM] sofia.rodriguez
Agreed. Our job is to find the truth and report it accurately. What they do with
it is their decision.

[5:02 PM] maria.santos
For what it's worth, I think you're doing the right thing Sofia. It takes courage
to look for problems you might not want to find.

[5:04 PM] alex.chen
+1. Better we find it than someone else.

[5:06 PM] sofia.rodriguez
Thanks team. I'll keep you posted as I make progress. And if any of you hear
anything concerning from other teams or clients, let me know.

[5:08 PM] james.kim
Will do. Good luck with the analysis.

[5:09 PM] sofia.rodriguez
Thanks. I think we're going to need it.

================================================================================
[CHANNEL EXPORT ENDS]
================================================================================

Export metadata:
- Total messages in export: 47
- Date range: July 15-22, 2024
- Exported by: legal-dept@technova-ai.com
- Export timestamp: September 10, 2024 14:22 CET
- Export reason: Internal investigation / litigation preparation
- Legal review status: Pending
- Attorney-client privilege: Potentially applicable to portions

NOTES:
This export contains technical discussions among engineering team members that
may be relevant to:
1. Understanding when bias issues were first identified
2. Good faith efforts to identify and address compliance issues
3. Technical analysis of training data and model behavior
4. Awareness of legal and regulatory implications

Some messages may constitute:
- Trade secrets (technical methodologies)
- Attorney work product (if made in anticipation of litigation)
- Self-critical analysis (potentially privileged in some jurisdictions)

Legal review required before disclosure to external parties.

Classification: CONFIDENTIAL - ATTORNEY WORK PRODUCT
Custodian: TechNova Legal Department
File: Investigation/DataSure-Matter/Communications/Slack-Export-001